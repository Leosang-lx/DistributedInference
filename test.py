import pickle
import random
import sys
from queue import SimpleQueue

import matplotlib.pyplot as plt
import numpy as np
import torch



# labels = 'conv1 conv2 max1 conv3 conv4 max2 conv5 conv6 conv7 max3 conv8 conv9 conv10 max4 conv11 conv12 conv13 max5 linear1 linear2 linear'.split(' ')  # 18 21 24
# # labels = ['conv1', 'maxpool1', 'conv2', 'conv3', 'maxpool2', 'inception3a', 'inception3b',
# #           'maxpool3', 'inception4a', 'inception4b', 'inception4c', 'inception4d', 'inception4e',
# #           'maxpool4', 'inception5a', 'inception5b', 'avgpool', 'dropout', 'linear']
# xx = [*list(range(18)), 18, 21, 24]
# times = [[0.3121635913848877, 5.998010873794556, 0.014268636703491211, 2.846705913543701, 5.681143045425415, 0.007390022277832031, 3.231539487838745, 6.364696264266968, 6.3380043506622314, 0.0032126903533935547, 3.183817148208618, 6.142641544342041, 6.111390590667725, 0.0018167495727539062, 1.1653335094451904, 1.1655399799346924, 1.1654751300811768, 0.0002925395965576172, 0.21370482444763184, 0.0001239776611328125, 0.00046443939208984375, 0.03511643409729004, 0.00013709068298339844, 0.0003273487091064453, 0.008695363998413086], [0.2956562042236328, 6.044829607009888, 0.012134313583374023, 2.850778102874756, 5.885300636291504, 0.006440639495849609, 3.2066562175750732, 6.242631435394287, 6.2998011112213135, 0.0033104419708251953, 3.0422043800354004, 6.10520339012146, 6.108281373977661, 0.0017704963684082031, 1.1652188301086426, 1.165633201599121, 1.165280818939209, 0.0002887248992919922, 0.21373701095581055, 0.0001270771026611328, 0.0003223419189453125, 0.03508758544921875, 0.00012993812561035156, 0.0003197193145751953, 0.008715152740478516], [0.2892742156982422, 6.2088212966918945, 0.011921405792236328, 2.846538782119751, 5.713587999343872, 0.005970001220703125, 3.2102959156036377, 6.352908611297607, 6.257172107696533, 0.003347158432006836, 3.0950117111206055, 6.0782835483551025, 6.112911939620972, 0.0017771720886230469, 1.1653456687927246, 1.1653358936309814, 1.1653692722320557, 0.0003018379211425781, 0.21378540992736816, 0.0001289844512939453, 0.00033354759216308594, 0.035170793533325195, 0.00012993812561035156, 0.00032448768615722656, 0.008693695068359375]]
# # times = [[0.3903510570526123, 0.0043544769287109375, 0.026010990142822266, 1.1896111965179443, 0.0025534629821777344, 0.22126030921936035, 0.47862863540649414, 0.0017635822296142578, 0.13360857963562012, 0.1557779312133789, 0.1771407127380371, 0.20816254615783691, 0.29201269149780273, 0.0009341239929199219, 0.10800719261169434, 0.14876914024353027, 0.0004220008850097656, 0.00028896331787109375, 0.002389192581176758], [0.3709380626678467, 0.003416299819946289, 0.024808168411254883, 1.198838233947754, 0.0024712085723876953, 0.22120428085327148, 0.47784423828125, 0.0013744831085205078, 0.13279986381530762, 0.15438151359558105, 0.17629170417785645, 0.2076258659362793, 0.2910282611846924, 0.0007150173187255859, 0.10749530792236328, 0.1479177474975586, 0.00031828880310058594, 0.00018715858459472656, 0.0023446083068847656], [0.3706240653991699, 0.0032744407653808594, 0.02477717399597168, 1.2110669612884521, 0.0027217864990234375, 0.22124052047729492, 0.476207971572876, 0.0014843940734863281, 0.13273096084594727, 0.15446233749389648, 0.17600750923156738, 0.20708346366882324, 0.2909209728240967, 0.0006737709045410156, 0.1081535816192627, 0.14800667762756348, 0.0003178119659423828, 0.00018310546875, 0.0023221969604492188], [0.3710598945617676, 0.0035429000854492188, 0.024569034576416016, 1.2184607982635498, 0.002610445022583008, 0.22124719619750977, 0.4763803482055664, 0.0014047622680664062, 0.1327042579650879, 0.15459108352661133, 0.17609596252441406, 0.20714116096496582, 0.29052066802978516, 0.0007338523864746094, 0.10753989219665527, 0.1478724479675293, 0.00031495094299316406, 0.00018358230590820312, 0.002283334732055664], [0.36915063858032227, 0.003470182418823242, 0.024930715560913086, 1.2207412719726562, 0.0027141571044921875, 0.2212378978729248, 0.47641539573669434, 0.0014977455139160156, 0.1327073574066162, 0.1549530029296875, 0.1759653091430664, 0.20729780197143555, 0.2908649444580078, 0.000682830810546875, 0.10785269737243652, 0.14791107177734375, 0.0003192424774169922, 0.00018668174743652344, 0.002337932586669922]]
# t = np.asarray(times).mean(axis=0)
# t = t[xx]
# print(t)
# plt.figure(figsize=(12,3))
# plt.grid(axis='y', ls='--', zorder=0)
# plt.bar(labels, t, width=0.3, color='orange', zorder=10)
# plt.xticks(rotation=30, fontsize=12)
# plt.yticks(fontsize=12)
# plt.ylabel('Latency (s)', fontsize=12)
# # plt.xlabel('Layers', fontsize=8)
# plt.tight_layout()
# file_name = 'vgg16_latency'
# plt.savefig('D:/华为云盘/毕设/final/figures/' + file_name + '.pdf', bbox_inches='tight')
# plt.show()


# from paint import show_transmission_size
# bod3 = [0, 0, 0, 0, 1204224, 200704, 0, 0, 0, 100352, 0, 100352, 401408, 0, 0, 0, 301056, 0, 200704, 752640, 150528, 0, 0, 0, 37632, 0, 50176, 125440, 0, 0, 0, 50176, 0, 50176, 100352, 0, 0, 0, 50176, 0, 50176, 87808, 0, 0, 0, 50176, 0, 50176, 200704, 0, 0, 0, 100352, 0, 100352, 326144, 50176, 0, 0, 0, 25088, 0, 25088, 75264, 0, 0, 0, 25088, 0, 25088, 1605632, 0, 802816, 802816, 802816, 827904, 0, 326144, 0]
# mpbd3 = [28672, 0, 28672, 43008, 602112, 200704, 21504, 0, 0, 100352, 0, 100352, 401408, 28672, 0, 0, 301056, 0, 200704, 376320, 150528, 10752, 0, 0, 37632, 0, 50176, 125440, 12544, 0, 0, 50176, 0, 50176, 100352, 14336, 0, 0, 50176, 0, 50176, 87808, 16128, 0, 0, 50176, 0, 50176, 200704, 17920, 0, 0, 100352, 0, 100352, 163072, 50176, 8960, 0, 0, 25088, 0, 25088, 75264, 10752, 0, 0, 25088, 0, 25088, 802816, 53760, 401408, 401408, 401408, 413952, 46592, 163072, 0]
# pcc3 = [143360, 0, 57344, 129024, 86016, 136192, 43008, 272384, 7168, 68096, 0, 68096, 272384, 57344, 408576, 14336, 204288, 0, 136192, 107520, 96768, 21504, 104832, 3584, 24192, 0, 32256, 80640, 25088, 112896, 5376, 32256, 0, 32256, 64512, 28672, 129024, 5376, 32256, 0, 32256, 56448, 32256, 145152, 7168, 32256, 0, 32256, 129024, 35840, 161280, 7168, 64512, 0, 64512, 93184, 35840, 17920, 44800, 3584, 17920, 0, 17920, 53760, 21504, 53760, 5376, 17920, 0, 17920, 630784, 1021440, 344064, 344064, 344064, 354816, 465920, 186368, 0]
# ppc3 = [143360, 0, 57344, 129024, 86016, 0, 43008, 0, 7168, 0, 0, 0, 0, 57344, 0, 14336, 0, 0, 0, 107520, 0, 21504, 0, 3584, 0, 0, 0, 0, 25088, 0, 5376, 0, 0, 0, 0, 28672, 0, 5376, 0, 0, 0, 0, 32256, 0, 7168, 0, 0, 0, 0, 35840, 0, 7168, 0, 0, 0, 93184, 0, 17920, 0, 3584, 0, 0, 0, 0, 21504, 0, 5376, 0, 0, 0, 114688, 268800, 114688, 114688, 114688, 118272, 139776, 93184, 0]
# assert len(bod3) == len(mpbd3) == len(pcc3) == len(ppc3)
# xs = list(range(len(ppc3)))
# show_transmission_size([bod3, mpbd3, pcc3, ppc3], ['BOD', 'MPBD', 'PCC', 'PPC'], 'transmission_size')
# a = 3551201
# b = 0.0654153 + 0.05106966 + 0.02884054
# print(b)
# a = 0.9186276
# b = 0.03401645 + 0.01035833 + 0.01231384
# print(b)
# x = torch.randn((1,2,3,3))
# layer = nn.Conv2d(2, 2, 3, 1, 1)
# weight = layer.weight
# print(weight.shape)
# out = layer(x)
# print(out.shape)
# weight1 = weight[:1]
# weight2 = weight[1:]
# out1 = F.conv2d(x, weight1, stride=1, padding=1)
# out2 = F.conv2d(x, weight2, stride=1, padding=1)
# out_ = torch.concat([out1, out2], dim=1)
# print(out)
# print(out_)
# print(out_.shape)
# print(torch.allclose(out, out_))
# from models.googlenet import BasicConv2d

# def product(num_list):
#     res = 1
#     for i in num_list:
#         res *= i
#     return res
#
#
# shape = [1, 64, 300, 150]
# a = torch.randn(shape)
#
# print(4 * product(shape))
# print(sys.getsizeof(a.storage()))
# print(len(pickle.dumps(a)) / 1024)
# b = a[..., -1:].clone().detach()
# print(b.shape)
# print(sys.getsizeof(b.storage()))
# print(len(pickle.dumps(b)) / 1024)


# layer = BasicConv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
# last_output = torch.randn((1, 16, 28, 28))
# weight = layer.conv.weight
# partition = [0, 14, 28]
# paddings = [(1,0,1,1), (0,1,1,1)]
# stride = (1,1)
# require = [(0, 15), (13, 28)]
# worker = 0, 1
#
# correct_conv_output = layer.conv(last_output)
# correct_conv_output1 = correct_conv_output[..., 0:14]
# correct_output = layer(last_output)
# correct_output1 = correct_output[..., 0:14]
#
#
# x1 = last_output[..., 0:15]
# x1 = F.pad(x1, paddings[1])  # padding
# output1 = F.conv2d(x1, weight, stride=stride, padding=0)
# print(torch.equal(output1,correct_conv_output1))
# output1 = F.relu(output1, inplace=True)
# print(torch.equal(output1, correct_output1))

# q = SimpleQueue()
# print(q.get())


# import asyncio
# import aiohttp
#
# host = 'https://www.baidu.com'
# # urls_todo = {'/', '/1', '/2', '/3', '/4', '/5', '/6', '/7', '/8', '/9'}
#
# loop = asyncio.get_event_loop()
#
#
# async def fetch(url):
#     async with aiohttp.ClientSession(loop=loop) as session:
#         async with session.get(url) as response:
#             response = await response.read()
#             return response
#
#
# if __name__ == '__main__':
#     import time
#     start = time.time()
#     tasks = [fetch(host) for _ in range(10)]
#     res = loop.run_until_complete(asyncio.gather(*tasks))
#     print(time.time() - start)
#     print(res)



